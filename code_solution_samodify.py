# -*- coding: utf-8 -*-
"""Copy of BaselineSolution_SAmodify.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TZpYciDcGPLeu-TxWtlzY17gymQngCQ5

Baseline method: 
https://github.com/ITU-AI-ML-in-5G-Challenge/ITU-ML5G-Slidin-Videos-Baseline-Solution/blob/main/BaselineSolution.ipynb

# Slidin' Videos: Use high-precision text tracking and semantic segmentation for chapters generation

#### Please register for the Slidin' Videos challenge to get download URLs used in this notebook

## 0. Data preparation - video to frames generation
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/gdrive')
# %ls
# %mkdir Images

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/gdrive/MyDrive/dataset.part01/DwoMBc1PRZWgC19dblEhZw%3D%3D_2
# %ls

from IPython.core.display import display_pdf
# Program To Read video
# and Extract Frames
import cv2
import os

# Function to extract frames
def FrameCapture(path):
	
	# Path to video file
	vidObj = cv2.VideoCapture(path)

	# Used as counter variable
	count = 0

	# checks whether frames were extracted
	success = 1

	while success:

		# vidObj object calls read
		# function extract frames
		success, image = vidObj.read()
    
		# Saves the frames with frame-count	
		img_source = os.path.join("/content/Images", str(count) + "_slide_00_00_timestamp.jpg") 
		cv2.imwrite(img_source, image)
		#print(img_source)
		count += 1

# Driver Code
if __name__ == '__main__':

	# Calling the function
	FrameCapture("video.mp4")


#1_noslide_00_00_00.000.jpg

"""## Read CSV file 
# for training and testing
"""

# Python program to read CSV file line by line
# import necessary packages
import csv
  
# Open file 
with open('groundtruth.csv') as file_obj:
      
    # Create reader object by passing the file 
    # object to reader method
    reader_obj = list(csv.reader(file_obj))[1:]

    # Iterate over each row in the csv 
    # file using reader object
    for row in reader_obj:
        #print(row)
        train = (80 * (int(row[2])-int(row[0]))/100) 
        test = int(row[2]) - train
        print("train:")



"""## 1. Deeplab finetuning
#### DeepLabV3 model is a pretrained model for semantic segmentation of images. We will finetune it on Slidin' Videos dataset of slide titles
"""

git clone https://github.com/msminhas93/DeepLabv3FineTuning.git
pip install -r DeepLabv3FineTuning/requirements.txt

"""#### Download title masks collection and unzip it to the cloned repository. """

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/gdrive')
# %ls

wget -O slidin_videos_title_segmentation.zip $TITLEMASKS_URL
unzip -q slidin_videos_title_segmentation.zip -d DeepLabv3FineTuning/

"""#### Training DeepLab model: our goal is to maximize test_f1_score
#### You may want to play with number of epochs, learning rate and loss function to get better results
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd DeepLabv3FineTuning
from pathlib import Path

#from segmentation_models_pytorch.losses.jaccard  import JaccardLoss
#from segmentation_models_pytorch.losses.constants import BINARY_MODE
#criterion = JaccardLoss(mode=BINARY_MODE)

import torch
from sklearn.metrics import f1_score, roc_auc_score
from torch.utils import data

import datahandler
from model import createDeepLabv3
from trainer import train_model

# Create the deeplabv3 resnet101 model which is pretrained on a subset
# of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.
model = createDeepLabv3()
model.train()


data_directory = Path("/content/DeepLabv3FineTuning")#Path("TitleSegmentationDataset") 
# Create the experiment directory if not present
exp_directory = Path("DemoExp")
if not exp_directory.exists():
    exp_directory.mkdir()

batch_size = 4
epochs = 10 

# Specify the evaluation metrics
metrics = {'f1_score': f1_score, 'auroc': roc_auc_score}

# Create the dataloader
dataloaders = datahandler.get_dataloader_sep_folder(
    data_directory, image_folder="Images", mask_folder="Masks", batch_size=batch_size)



# Special ClassRoom loss function from 
# Prasad, S., D. Lin, Y. Li, S. Dong, T. L. Nwe., 2021. A Progressive Multi-view Learning Approach for Multi-loss Optimization in 3D Object Recognition. In IEEE Signal Processing Letters, vol. 29, pp. 707-711, 2022, doi: 10.1109/LSP.2021.3132794 
students_1 = torch.nn.MSELoss(reduction='mean')
students_2 = torch.nn.MSELoss(reduction='sum')
students_3 = torch.nn.HingeEmbeddingLoss(reduction='mean')
students_4 = torch.nn.HingeEmbeddingLoss(reduction='sum')
students_5 = torch.nn.L1Loss(reduction='sum')
students_6 = torch.nn.L1Loss(reduction='mean')
criterion = [students_1, students_2, students_3, students_4, students_5, students_6]

# Specify the optimizer with a lower learning rate
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
_ = train_model(model, 
                criterion,
                dataloaders,
                optimizer,
                bpath=exp_directory,
                metrics=metrics,
                num_epochs=epochs)

# Save the trained model
torch.save(model, exp_directory / 'weights.pt')

# Terminate
exit()
# %cd ../

"""## 2. Data Preprocessing
#### Download testset presentations for benchmarking
"""

wget -O slidin_videos_testset.zip $TESTSET_URL
unzip -q slidin_videos_testset.zip

"""#### The original resolution of the videos is too high, let's resize it"""

apt install ffmpeg
for f in testset/* ; do ffmpeg -v quiet -stats -i "$f/video.mp4" -crf 18 -c:a copy -vf scale=1280:-2 -y "$f/resized.mp4"  ;  done

"""#### Install `slideannot` package and pre-scan every video in the testset"""

wget -O slideannot.zip https://ituint-my.sharepoint.com/:u:/g/personal/kirill_ekshembeev_itu_int/EbehHvPSwn1Jrs6Sgr16upwBumBCpXS0sayQMP1uCMwoDg?download=1
unzip -q slideannot.zip
pip install -r slideannot/requirements.txt

"""#### Every text change (state) detected by `scan_video` will be saved to a separate folder with relevant OCR data (coordinates of textboxes and its text)
* state['start_idx'] - starting frame number of the state
* state['start_frame'] - numpy array containing actual frame
* state['end_idx'] - frame number of when this state ends (with a new state starting on a next frame)
* state['end_frame'] - ending and starting frames are identical in terms of textual content, but we would like to keep it both for reference
* state['boxes'] - bounding box of every textbox detected at start_frame
* state['texts'] - text recognized by OCR for each bounding box
"""

from slideannot.scan import scan_video

import os
import tqdm
import cv2
import json


def save(state, save_path):
    start, end = state['start_idx'], state['end_idx']   
    os.makedirs(f'{save_path}/changes/{start}_{end}')   
    
    cv2.imwrite(f'{save_path}/changes/{start}_{end}/{start}.jpg', state['start_frame'])   
    cv2.imwrite(f'{save_path}/changes/{start}_{end}/{end}.jpg', state['end_frame'])  
    
    boxes = [ {
        'bbox': [ int(p) for p in bbox ],
        'width': int(bbox[3] - bbox[2]),
        'height': int(bbox[1] - bbox[0]),
        'text': text,
        'fontsize': 1.0 * int(bbox[3] - bbox[2]) / len(text),
    } for bbox, text in zip(state['boxes'], state['text']) ]
    
    with open(f'{save_path}/ocr/{start}.json', 'w') as f:
        f.write(json.dumps({ 'boxes': boxes }))        

videos_dir = 'testset'
data_dir = 'data'
        
for pres_id in os.listdir(videos_dir):
    video_path = f'{videos_dir}/{pres_id}/resized.mp4'
    save_path = f'{data_dir}/{pres_id}'
    
    if os.path.exists(save_path):
        os.system(f'rm -r  {save_path};') 
        
    os.makedirs(f'{save_path}/changes')
    os.makedirs(f'{save_path}/ocr')
        
    for state in scan_video(video_path, pbar=tqdm.tqdm()):
        save(state, save_path)
        
exit()

"""## 3. Explore the solution in depth and benchmark it
#### Import the requirements and define helper functions
"""

from slideannot.helpers import sort_boxes, resize_boxes
from slideannot.metrics import accuracy
        
from torchvision import transforms  
import pandas as pd       
import os
import json
import tqdm
import cv2    
from PIL import Image
import torch
import numpy as np
    
def parse_state(dirname, path):
    begin, end = dirname.split('_')
    return { 'begin': int(begin), 'end': int(end), 
        'begin_frame': f'{path}/changes/{dirname}/{begin}.jpg',
        'end_frame': f'{path}/changes/{dirname}/{end}.jpg',
        'ocr_path': f'{path}/ocr/{begin}.json', }

def list_changes(path):
    return sorted([ parse_state(dirname, path) 
                    for dirname in os.listdir(f'{path}/changes') 
                    if '_' in dirname ], key=lambda state: state['begin'])   

data_dir = 'data'
input_dir = 'testset'

presentations = [ pres_id for pres_id in os.listdir(data_dir) 
                if os.path.exists(f'{data_dir}/{pres_id}/changes') ]

"""#### Load finetuned deeplab model"""

deeplab_width = 768
deeplab_height = 432
deeplab = torch.load('DeepLabv3FineTuning/DemoExp/weights.pt')
deeplab.eval()

def boxify(out, boxes):
    return [ torch.mean(out[b[0]:b[1], b[2]:b[3]]).item()  for b in boxes ]      

def titlesegment(frame, boxes):
    # Resize the frame to the training input size
    resized = cv2.resize(frame, (deeplab_width, deeplab_height))
    inputs = transforms.ToTensor()(Image.fromarray(resized, mode="RGB"))
    inputs = inputs.reshape(1, *inputs.shape).to('cuda')
    
    # Resize box coordinates to fit resized frame
    boxes = resize_boxes(boxes, orig_size=frame.shape, 
                         new_size=(deeplab_height, deeplab_width))
    
    with torch.set_grad_enabled(False):
        outputs = deeplab(inputs)
        out = outputs['out'][0][0]
        box_scores = boxify(out, boxes)
        return out, np.array(box_scores)

"""#### During this step we will choose one of the pre-scanned presentations and examine outputs of a scanning algorithm. `slideannot.scan_video` function segments video based on textual changes (it records a new state every time text appears or disappears in the video) """

from IPython.display import display, JSON, Image as IImage

def draw_boxes(frame, boxes):
    for box in boxes:
        frame = cv2.rectangle(frame, (box[2], box[0]), (box[3], box[1]), (36,255,12), 2)
    return frame

demo_id = presentations[-2]
demo_states = list_changes(f'{data_dir}/{demo_id}')

# Pick states #346 and #347 for inspection
state_before, state_after = demo_states[346], demo_states[347]

ocr_before = pd.read_json(state_before['ocr_path'])
boxes_before = [ box['bbox'] for box in ocr_before['boxes'] ]

frame_before = cv2.imread(state_before['end_frame'])
frame_before_debug = draw_boxes(frame_before,  boxes_before)

display(f"Presentation ID: {demo_id}, State #346: Frames {state_before['begin']}-{state_before['end']}", 
    Image.fromarray(frame_before_debug).resize((640,360)), pd.DataFrame(list(ocr_before['boxes'])))

"""#### There seems to be several textboxes appeared at a frame **#7850** (state **#347**)"""

ocr_after = pd.read_json(state_after['ocr_path'])['boxes']
data_after = pd.DataFrame(list(ocr_after))
boxes_after = [ box['bbox'] for box in ocr_after ]

frame_after = cv2.imread(state_after['begin_frame'])
frame_after_debug = draw_boxes(frame_after.copy(),  boxes_after)

display(f"Presentation ID: {demo_id}, State #347: Frames {state_after['begin']}-{state_after['end']}",
    Image.fromarray(frame_after_debug).resize((640,360)), data_after.head(5))

"""#### Let's use deeplab model to perform sample title segmentation of a state **#347** . To achieve this, we will calculate "title probability" for each textbox in green"""

out, scores = titlesegment(frame_after, boxes_after)
out_mask = (np.abs(out.cpu().numpy())*255).astype(np.uint8)

data_after['score'] = scores

display(f"Presentation ID: {demo_id}, State #347: Frames {state_after['begin']}-{state_after['end']}",
    Image.fromarray(out_mask).resize((640,360)), data_after.head(5))

"""#### OCR algorithms can be incosistent when "reading" same set of characters with slight visual differences. To properly track slide titles throughout presentation, we will be fuzzy matching titles of different states to aggregate them as a *single slide state*"""

from Levenshtein import distance

def sametitle(t1, t2):
    t1 =  ''.join([ c for c in t1.lower() if c.isalnum() ])
    t2 =  ''.join([ c for c in t2.lower() if c.isalnum() ])
    return distance(t1, t2) <= 1

"""#### Final step of the solution: segment a title of each state, aggregate states with the same title and output a dataframe with predictions"""

def occupation(boxes):
    area = lambda b: (b[1]-b[0]) * (b[3]-b[2])
    y1, y2 = min([b[0] for b in boxes ]), max([b[1] for b in boxes ])
    x1, x2 = min([b[2] for b in boxes ]), max([b[3] for b in boxes ])
    return 1.0 * sum([ area(b) for b in boxes ]) / area([y1, y2, x1, x2])


def pickonetitle(boxes, scores):
    if len(boxes) == 0:
        return []
        
    best_idx = np.argmax(scores)
    best = boxes[best_idx]

    candidates = { i for i, box in enumerate(boxes) 
        if abs(box['fontsize']-best['fontsize']) <= best['fontsize'] * 0.25 
            and scores[best_idx] - scores[i] < 0.6 }
    
    while occupation([ boxes[i]['bbox'] for i in candidates ]) < 0.75:
        combinations = [ candidates - {j} for j in candidates if j != best_idx ]
        candidates = max(combinations, 
            key=lambda comb: occupation([ boxes[j]['bbox'] for j in comb ]))
            
    title_boxes = [ boxes[i] for i in candidates ]
    return title_boxes


log = []

for pres_id in presentations:
    data_path = f'{data_dir}/{pres_id}'
    groundtruth_path = f'{input_dir}/{pres_id}/groundtruth.csv'
    
    slides = []

    for state in list_changes(data_path):
        # If state is too short, it's probably(!) not worth analyzing
        if state['end'] - state['begin'] < 25:
            continue
        
        frame = cv2.imread(state['begin_frame']) 
        ocr = pd.read_json(state['ocr_path'])
        boxes = [ box for box in ocr['boxes'] if box['bbox'][1] > 0.19*frame.shape[0] or box['bbox'][2] < 0.81 * frame.shape[1] ] 
        
        out, boxes_scores = titlesegment(frame, [box['bbox'] for box in boxes])
        title_boxes = pickonetitle(boxes, boxes_scores)
        
        # Sort text boxes as you would read it in English: from top to bottom
        # and from left to right
        title_boxes = sort_boxes(title_boxes)
        title = ' '.join(box['text'] for box in title_boxes)

        # Possibly no text on a screen, we should either skip this
        # segment or mark it as a no title case
        if len(title_boxes) == 0:
            continue
        
        if len(slides) == 0 or not sametitle(slides[-1]['title1'], title):
            slides.append({ 'starting_frame': state['begin'], 'ending_frame': state['end'], 
                            'title1': title, 'title2': None, 'title3': None, 'title4': None })

        slides[-1]['ending_frame'] = state['end']
        
    slides = [ slide for slide in slides if slide['ending_frame'] - slide['starting_frame'] >= 50 ]
        
    predicted = pd.DataFrame(slides)
    groundtruth = pd.read_csv(groundtruth_path)
    
    acc = accuracy(predicted, groundtruth)
    
    print(f'Presentation ID: {pres_id}')
    print(f"Final Acc: {acc['fa']}, Boundary Acc: {acc['ba']}, Title Acc: {acc['ta']}")
    
    log.append(acc)

predicted

groundtruth